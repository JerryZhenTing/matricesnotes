\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}

\title{\Huge{36225}\\Probability}
\author{\huge{ZhenTing Liu}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{Multivariate Distribution}
\section{Multivariate distribution}
\dfn{}{Let $Y_1$ and $Y_2$ be discrete random variables. The joint (or bivariate) probability
function for $Y_1$ and $Y_2$ is given by
$$p(y_1,y_2) = P(Y_1 = y_1, Y_2 = y_2), \quad -\infty < y_1 < \infty, -\infty < y_2 < \infty$$
}

In the single-variable case , the probability function for a discrete random variable Y assigns nonzero probabilities to a finite or
countable number of distinct values of Y in such a way that the sum of the probabilities
is equal to 1. Similarly, in the bivariate case the joint probability function $p(y_1, y_2)$
assigns nonzero probabilities to only a finite or countable number of pairs of values
$(y_1, y_2)$. Further, the nonzero probabilities must sum to 1.

\thm{Discrete random variables}{
	If $Y_1$ and $Y_2$ are discrete random variables with joint probability function
$p(y_1, y_2)$, then
\begin{enumerate}
	\item  $p(y_1,y_2) \geq 0 \quad \forall y_1, y_2 $
	\item $\sum_{y_1,y_2}p(y_1,y_2) = 1$
\end{enumerate}
}

As in the case of univariate random variables, the distinction between jointly
discrete and jointly continuous random variables may be characterized in terms of
their (joint) distribution functions.

\dfn{}{For any random variables $Y_1$ and $Y_2$, the joint (bivariate) distribution function
$F(y_1, y_2)$ is
$$F(y_1,y_2) = P(Y_1 \leq y_1, Y_2 \leq y_2) \quad -\infty < y_1 < \infty, -\infty < y_2 < \infty$$
}

\nt{
	If $Y_1$ and $Y_2$ are random variables with joint distribution function $F(y_1, y_2)$, then
	\begin{enumerate}
		\item $F(-\infty, -\infty) = F(-\infty,y_2) = F(y_1, -\infty) = 0$
		\item $F(\infty,\infty) = 1$
	\end{enumerate}
}

\thm{Continous random variables}{If $Y_1$ and $Y_2$ are jointly continuous random variables with a joint density function
given by $f(y_1, y_2)$, then
\begin{enumerate}
	\item $f(y_1,y_2) \geq 0, \quad \text{for all} \quad y_1, y_2$
	\item $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(y_1,y_2)dy_1dy_2 = 1$
\end{enumerate}

}




\section{Marginal and Conditional distribution}
univariate event $(Y_1 = y_1)$ is the union of bivariate events of the type
$(Y_1 = y_1, Y_2 = y_2)$, with the union being taken over all possible values for $y_2$.
\dfn{Marginal Distribution}{Let $Y_1$ and $Y_2$ be jointly discrete random variables with probability function
$p(y_1, y_2)$. Then the marginal probability functions of $Y_1$ and $Y_2$, respectively,
are given by
$$p_1(y_1) = \sum_{\text{all} y_2}p(y_1,y_2) \quad \text{and} \quad p_2(y_2) = \sum_{\text{all}y_1}p(y_1,y_2) $$
Let $Y_1$ and $Y_2$ be jointly continuous random variables with joint density function
$f(y_1, y_2)$. Then the marginal density functions of $Y_1$ and $Y_2$, respectively, are
given by
$$f_1(y_1) = \int_{-\infty}^{\infty}f(y_1,y_2)dy_2 \quad \text{and} \quad f_2(y_2)= \int_{-\infty}^{\infty}f(y_1,y_2)dy_1$$
}

The term \textbf{marginal}, as applied to the univariate probability functions of $Y_1$ and $Y_2$
, has intuitive meaning. To find $p_1(y_1)$, we sum $p(y_1,y_2)$ over all values of $y_2$, and hence
accumulate the probabilities on the $y_1$ axis (or margin). 

\nt{To find marginal pdf
  \begin{enumerate}
    \item integrate over the other variable
    \item fix a line corresponding to a value of desired marginal pdf
    \item find the limit of integral: intersecting points between this line and support of the joint pdf
    \item check if the boundary of support is straight line
    \item remember to specify the support of the resulting marginal pdf

  \end{enumerate}
}

\dfn{Conditional Distribution discrete case}{
If $Y_1$ and $Y_2$ are jointly discrete random variables with joint probability function
$p(y_1, y_2)$ and marginal probability functions $p_1(y_1)$ and $p_2(y_2)$, respectively,
then the conditional discrete probability function of $Y_1$ given $Y_2$ is

$$p(y_1 | y_2) = P(Y_1 = y_1 | Y_2 = y_2) = \frac{P(Y_1 = y_1, Y_2 = y_2)}{P(Y_2 = y_2)} = \frac{p(y_1,y_2)}{p_2(y_2)}$$

}


\dfn{Conditional Distribution cont. case}{
If $Y_1$ and $Y_2$ are jointly continuous random variables with joint density function
$f(y_1, y_2)$ and marginal densities $f_1(y_1)$ and $f_2(y_2)$ respectively, then the conditional distribution function of $Y_1$ given $Y_2 = y_2$ is

$$F(y_1 | y_2) = P(Y_1 \leq y_1 | Y_2 = y_2)$$
$$ = \int_{-\infty}^{y_1}\frac{f(t_1, y_2)}{f_2(y_2)}dt_1$$
\text{We will call the integrand of this expression the conditional density function of $Y_1$
given $Y_2 = y_2$}
\text{and we will denote it by $f(y_1|y_2)$.}

Note for any $f_2(y_2) > 0$, the conditional density of $Y_1$ given $Y_2 = y_2$ is given by

$$f(y_1 | y_2) = \frac{f(y_1,y_2)}{f_2(y_2)}$$
and, for any $f_1(y_1) > 0$, the conditional density of $Y_2$ given $Y_1 = y_1$
is gievn by
$$f(y_2 | y_1) = \frac{f(y_1,y_2)}{f_1(y_1)}$$


}
Notice that $F(y_1|y_2)$ is a function of $y_1$ for a fixed value of $y_2$
\nt{
	$$P(a < Y_1 < b | Y_2 = y_2) = \int_{a}^{b}f(y_1 | y_2)dy_1$$
	also for a valid pmf/pdf for $Y_1$ or $Y_2$ as univariate r.v.
	\begin{enumerate}
		\item $f(y_1 | y_2) = \frac{f(y_1, y_2)}{f_2(y_2)} \geq 0$
		\item $\int_{-\infty}^{\infty}f(y_1 | y_2)dy_1 = \int_{-\infty}^{\infty}\frac{f(y_1,y_2)}{f_2(y_2)}dy_1 = 1$
	\end{enumerate}
	}
\subsection[Examples]{Examples}
\[
f(y_1, y_2) =
\begin{cases} 
30y_1 y_2^2, & y_1 - 1 \leq y_2 \leq 1 - y_1, \, 0 \leq y_1 \leq 1, \\
0, & \text{elsewhere}.
\end{cases}
\] 
\qs{
}{
	a \quad show that the marginal density of $Y_1$ is a beta density with $\alpha = 2$ and $\beta = 4$
	\\
	b \quad Derive the marginal density of $Y_2$
	\\
	c \quad Derive the conditional density of $Y_2$ given $Y_1 = y_1$
	\\
	d \quad Find $P(Y_2 > 0 | Y_1 = 0.75)$
}
\sol
\\ 
a \quad 
\[
f_1(y_1) = 
\begin{cases}
	\int_{y_1 - 1}^{1-y_1}30y_1y_{2}^{2}dy_2, & \, 0 \leq y_1 \leq 1 \\
	0, & \text{elsewhere}.
\end{cases}
\]
$\sim$ Beta(2,4) after integration \\
b \quad 

\[
f_2(y_2) = 
\begin{cases}
	\int_{0}^{1-y_2}30y_1y_{2}^{2}dy_1, & \, 0 \leq y_2 \leq 1 \\
	\int_{0}^{y_2+1}30y_1y_{2}^{2}dy_1, & \, -1 \leq y_2 \leq 0  \\
	0, & \text{elsewhere}.
\end{cases}
\]
\\
c \quad
$$f(y_2|y_1) = \frac{f(y_1,y_1)}{f_1(y_1)} = \frac{30y_1y_2^2}{20y_1(1-y_1)^3} = \frac{3y_2^2}{2(1-y_1)^3} \quad y_1 -1 \leq y_2 \leq 1-y_1, 0 \leq y_1 \leq 1$$
\\
d \quad 
$$\int_{0}^{(1-0.75)}\frac{3y_2^2}{2(1-0.75)^3}dy_2 = \frac{1}{2}	$$
\section{Independence}
\dfn{The r.v. $Y_1, Y_2$ are independent if and only if }{discrete: $$p(y_1,y_2) = p_1(y_1)p_2(y_2) \quad \forall y_1,y_2 \in \mathbb{R}^2$$
coninuous: $$f(y_1,y_2) = f_1(y_1)f_2(y_2) \quad \forall y_1,y_2 \in \mathbb{R}^2$$
CDF: $$F(y_1,y_2) = F_1(y_1)F_2(y_2) \quad \forall y_1,y_2 \in \mathbb{R}^2$$}

\thm{Determining independence}{Given two r.v.'s $Y_1, Y_2$
Step 1: is the support of joint pmf/pdf rectangular with sides parallel to the axis? 
\begin{itemize}
	\item "No" $\implies$ Not independent 
	\item "Yes" $\implies$ Move to step 2
\end{itemize}
Step 2: Can the joint pmf/pdf be written as the product of the form $p(y_1,y_2) = h(y_a)g(y_b), a,b \in \{1,2\}$
\begin{itemize}
	\item "No" $\implies$ Not independent
	\item "Yes" $\implies$ Independent
\end{itemize}
tip: To rigorously prove "cannot be factorized", one way is to find one pair of $(y_1,y_2) \in \mathbb{R}^{2}$ such that
$$f_1(y_1)f_2(y_2) \neq f(y_1,y_2)$$
}
\section{Expected Value}
\dfn{}{
	Let \( g(Y_1, Y_2, \ldots, Y_k) \) be a function of the discrete random variables, \( Y_1, Y_2, \ldots, Y_k \), which have probability function \( p(y_1, y_2, \ldots, y_k) \). Then the \textit{expected value} of \( g(Y_1, Y_2, \ldots, Y_k) \) is
\[
E[g(Y_1, Y_2, \ldots, Y_k)] = \sum_{\text{all } y_k} \cdots \sum_{\text{all } y_2} \sum_{\text{all } y_1} g(y_1, y_2, \ldots, y_k) p(y_1, y_2, \ldots, y_k).
\]

If \( Y_1, Y_2, \ldots, Y_k \) are continuous random variables with joint density function \( f(y_1, y_2, \ldots, y_k) \), then
\[
E[g(Y_1, Y_2, \ldots, Y_k)] = \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty \int_{-\infty}^\infty
g(y_1, y_2, \ldots, y_k) f(y_1, y_2, \ldots, y_k) \, dy_1 \, dy_2 \cdots dy_k.
\]
}

\nt{In general, $E(Y_1Y_2) \neq E(Y_1)E(Y_2)$ except for when $Y_1$ and $Y_2$ are independent
\\
\textbf{Important: } Independence $\Rightarrow E(Y_1Y_2) = E(Y_1)E(Y_2)$ but $E(Y_1Y_2) = E(Y_1)E(Y_2) \nRightarrow$ Independence
	
}

Consider two random variables $Y_1, Y_2$ with density function $f(y_1,y_2)$. $$E(Y_1) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y_1 f(y_1,y_2)dy_2dy_1$$

\section{Covariance}
The covariance between two random variables $Y_1$ and $Y_2$, a measure of dependence between $Y_1, Y_2$, is defined as 
$$cov(Y_1, Y_2) = E[(Y_1 - \mu_1)(Y_2 - \mu_2)] \quad \mu_i = E(Y_i) \quad i = 1,2$$ 
which can also be written in shortcut form as
$$cov(Y_1,Y_2) = E(Y_1Y_2) - E(Y_1)E(Y_2)$$

note if $Y_1 = Y_2 = Y$ then $cov(Y,Y) = V(Y)$

\cor{}{if $Y_1, Y_2$ are independent, then $cov(Y_1,Y_2) = 0$ because $$E(Y_1Y_2) = E(Y_1)E(Y_2)$$ but $cov(Y_1Y_2) = 0$ does not imply independence}
The larger the absolute value of the covariance of $Y_1$ and $Y_2$ , the greater the
linear dependence between $Y_1$ and $Y_2$ . Positive values indicate that $Y_1$ increases as $Y_2$
increases; negative values indicate that $Y_1$ decreases as $Y_2$ increases. A zero value of
the covariance indicates that the variables are uncorrelated and that there is no linear
dependence between $Y_1$ and $Y_2$ 
\subsection[correlation coeff]{Correlation Coefficient}
\textsf{correlation coefficient} $\rho$ is a measurement of dependency and is defined as $$\rho = \frac{Cov(Y_1Y_2)}{\sigma_1\sigma_2}$$
where $\sigma_1, \sigma_2$ are the standard deviation of $Y_1, Y_2$ respectively.\\
\textbf{Note} $$-1 \leq \rho_{Y_1Y_2} \leq 1$$
\begin{itemize}
	\item $\rho = 0 \iff Y_1,Y_2$ uncorrelated
	\item $\rho = 1 \iff$ Perfect Positive linear dependence $Y_1 = aY_2 + b, \quad a>0$
	\item $\rho = -1 \iff$ Perfect Negative linear dependence $Y_1 = -aY_2 + b, \quad a>0$  
\end{itemize}
\pagebreak
\section{Linear Combination, Conditional Expectation}

\thm{}{
Let 
\( Y_1, Y_2, \ldots, Y_n \) and \( X_1, X_2, \ldots, X_m \) be random variables with \( E(Y_i) = \mu_i \) and \( E(X_j) = \xi_j \). Define
\[
U_1 = \sum_{i=1}^n a_i Y_i \quad \text{and} \quad U_2 = \sum_{j=1}^m b_j X_j
\]
for constants \( a_1, a_2, \ldots, a_n \) and \( b_1, b_2, \ldots, b_m \). Then the following hold:

\begin{enumerate}[label=\alph*.]
    \item \( E(U_1) = \sum_{i=1}^n a_i \mu_i. \)
    \item \( V(U_1) = \sum_{i=1}^n a_i^2 V(Y_i) + 2 \sum_{1 \leq i < j \leq n} a_i a_j \text{Cov}(Y_i, Y_j), \) where the double sum is over all pairs \( (i, j) \) with \( i < j \).
    \item \( \text{Cov}(U_1, U_2) = \sum_{i=1}^n \sum_{j=1}^m a_i b_j \text{Cov}(Y_i, X_j). \)
\end{enumerate}
}

\section{Conditional Expectation}
\dfn{}{
If \( Y_1 \) and \( Y_2 \) are any two random variables, the \textit{conditional expectation} of \( g(Y_1) \), given that \( Y_2 = y_2 \), is defined to be
\[
E(g(Y_1) \mid Y_2 = y_2) = \int_{-\infty}^\infty g(y_1) f(y_1 \mid y_2) \, dy_1
\]
if \( Y_1 \) and \( Y_2 \) are jointly continuous and
\[
E(g(Y_1) \mid Y_2 = y_2) = \sum_{\text{all } y_1} g(y_1) p(y_1 \mid y_2)
\]
if \( Y_1 \) and \( Y_2 \) are jointly discrete.
}
\ex{}{Consider random variable $Y_1, Y_2$ with joint density function 

\[
f(y_1,y_2) = 
\begin{cases}
	\frac{1}{2}, & \ 0 \leq y_1 \leq y_2 \leq 2 \\
	0, & \text{elsewhere}.
\end{cases}
\]
Find the conditional expectation of the amount of sales, $Y_1$, given that $Y_2 = 1.5$
}
\sol Note if $0 < y_2 \leq 2$,

\[
f(y_1|y_2) = 
\begin{cases}
	\frac{1}{y_2}, & \ 0 < y_1 \leq y_2  \\
	0, & \text{elsewhere}.
\end{cases}
\]
so for any value of $y_2$ such that $0<y_2 \leq 2$,
$$E(Y_1 | Y_2 = y_2) = \int_{-\infty}^{\infty}y_1 f(y_1 | y_2)dy_1$$
$$= \int_{0}^{y_2}y_1 \frac{1}{y_2}dy_1 = \frac{y_2}{2}$$

\thm{The Towering Rule}{
Let \( Y_1 \) and \( Y_2 \) denote random variables. Then
\[
E(Y_1) = E[E(Y_1 \mid Y_2)],
\]
where on the right-hand side the inside expectation is with respect to the conditional distribution of
 \( Y_1 \) given \( Y_2 \) 
 and the outside expectation is with respect to the distribution of \( Y_2 \).

}
\begin{myproof}

Suppose that 
\( Y_1 \) and \( Y_2 \) are jointly continuous with joint density function \( f(y_1, y_2) \) and marginal densities \( f_1(y_1) \) and \( f_2(y_2) \), respectively. Then
\[
E(Y_1) = \int_{-\infty}^\infty \int_{-\infty}^\infty y_1 f(y_1, y_2) \, dy_1 \, dy_2
\]
\[
= \int_{-\infty}^\infty \int_{-\infty}^\infty y_1 f(y_1 \mid y_2) f_2(y_2) \, dy_1 \, dy_2
\]
\[
= \int_{-\infty}^\infty \left\{ \int_{-\infty}^\infty y_1 f(y_1 \mid y_2) \, dy_1 \right\} f_2(y_2) \, dy_2
\]
\[
= \int_{-\infty}^\infty E(Y_1 \mid Y_2 = y_2) f_2(y_2) \, dy_2 = E[E(Y_1 \mid Y_2)].
\]

The proof is similar for the discrete case.

\end{myproof}
The conditional variance of $Y_1$ given $Y_2 = y_2$ is defined by analogy with an
ordinary variance, again using the conditional density or probability function of
$Y_1$ given $Y_2 = y_2$ in place of the ordinary density or probability function of $Y_1$.
That is,
$$V(Y_1|Y_2 = y_2) = E(Y_1^{2}|Y_2 = y_2) - [E(Y_1|Y_2 = y_2)]^2$$
\thm{}{
Let \( Y_1 \) and \( Y_2 \) denote random variables. Then
\[
V(Y_1) = E[V(Y_1 \mid Y_2)] + V[E(Y_1 \mid Y_2)].
\]
}

\begin{myproof}
As previously indicated, \( V(Y_1 \mid Y_2) \) is given by
\[
V(Y_1 \mid Y_2) = E(Y_1^2 \mid Y_2) - [E(Y_1 \mid Y_2)]^2
\]
and
\[
E[V(Y_1 \mid Y_2)] = E[E(Y_1^2 \mid Y_2)] - E\left\{[E(Y_1 \mid Y_2)]^2\right\}.
\]

By definition,
\[
V[E(Y_1 \mid Y_2)] = E\left\{[E(Y_1 \mid Y_2)]^2\right\} - \{E[E(Y_1 \mid Y_2)]\}^2.
\]

The variance of \( Y_1 \) is
\[
V(Y_1) = E[Y_1^2] - [E(Y_1)]^2
\]
\[
= E\{E[Y_1^2 \mid Y_2]\} - \{E[E(Y_1 \mid Y_2)]\}^2
\]
\[
= E[Y_1^2 \mid Y_2] - \{E[E(Y_1 \mid Y_2)]\}^2 + E\{[E(Y_1 \mid Y_2)]^2\} - \{E[E(Y_1 \mid Y_2)]\}^2
\]
\[
= E[V(Y_1 \mid Y_2)] + V[E(Y_1 \mid Y_2)].
\]
\end{myproof}
\chapter{Finals review lecture}
\section{Bivariate Distribution Functions}
\begin{itemize}
	\item joint pdf: $f(y_1,y_2)$
	\item Marginal pdf: $f_1(y_1) = \int_{-\infty}^{\infty}f(y_1,y_2)dy_2$
	\item Conditional pdf: $f(y_1 | y_2) = \frac{f(y_1,y_2)}{f_2(y_2)}$, \quad $y_1 \in D(y_2)$, (remember to specify domain)
	\item Independence: if $f(y_1,y_2) = f_1(y_1)f_2(y_2)$ for all $y_1,y_2 \in \mathbb{R}^2$, also the two-step procedure.
\end{itemize}
\section{Expectation and Covariance}
\begin{itemize}
	\item Expectation: $E(g(Y_1,Y_2)) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(y_1,y_2)f(y_1,y_2)dy_1dy_2$
	\item if $Y_1 \perp Y_2$, $E(h(Y_1)g(Y_2)) = E(h(Y_1))E(g(Y_2))$ 
	\subitem Q: $E(Y_1/Y_2) = E(Y_1)/E(Y_2)?$ \quad not true in general
	$E(Y_1\frac{1}{Y_2}) = E(Y_1)E(\frac{1}{Y_2})$
	\item $Cov(Y_1,Y_2) = E(Y_1Y_2) - E(Y_1)E(Y_2)$
	\item Correlation: $-1 \leq \rho = \frac{Cov(Y_1,Y_2)}{\sigma(Y_1)\sigma(Y_2) \leq 1}$
	\subitem independence $\implies$ uncorrelation but not the other way around
\end{itemize}
\section{Conditional Expectation and Variance}
\begin{itemize}
	\item $E(Y_1|Y_2 = y_2) = \int_{-\infty}^{\infty}y_1f(y_1|y_2)dy_1$ (is a function of $y_2$)
	\subitem then $E(Y_1|Y_2)$ is just a random variable because it is a function of r.v. $Y_2$
	\item Unconditional: $E(Y_1) = E(E(Y_1|Y_2))$, towering rule
	\subitem Q:$E(Y_1Y_2) = E(E(Y_1Y_2|Y_2)) = E(Y_2E(Y_1|Y_2))$, useful for $Cov(Y_1,Y_2)$
	\item $V(Y_1|Y_2 = y_2) = E(Y_1^2|Y_2 = y_2) - (E(Y_1|Y_2 = y_2))^2$
	\subitem $V(Y_1|Y_2)$ is a r.v.
	\item unconditional: $V(Y_1) = E(V(Y_1|Y_2)) + V(E(Y_1|Y_2))$
\end{itemize}
\section{Distribution of Functions of Random Variables}
\begin{itemize}
	\item Method of distribution functions 
	\subitem CDF: $F_u(u) = P(g(Y) \leq u)$
	\subitem PDF: $f_u(u) = \frac{dF_u(u)}{d_u}$
	\item Method of Transformation
	\subitem if $g$ is monotone on the support of $Y_1$, then $y = g^{-1}(u) \implies \frac{dg^{-1}(u)}{du}$ (doesn't have to be monotone across entire domain, just the support of $Y$)
	\subitem $f_u(u) = f_Y(g^-1(u))|\frac{dg^-1(u)}{du}|$
	\item Method of MGF
	\subitem $u = \sum_{i=1}^{n}a_iY_i + b$, $Y_i's$ are independent
	\subitem $m_u(0) = \prod_{i=1}^{n}m_{Y_{i}}(a_{i}t)e^{bt}$
	\subitem $M_Y(t) =E(e^{tY})$
	\subitem $E(Y^k) = |\frac{d^km_Y(t)}{dt^k}|_{t=0}$
\end{itemize}
\section{Sampling Distributions}
\begin{itemize}
	\item Exact Normality
	\item Approximation under Central Limit Theorem
\end{itemize}

% \section{Eigentheory}


% Let's begin by talking about why we care about this topic

% Consider the following System
% \begin{figure}
	
% \begin{center}
% 	\begin{tikzpicture}[->, >= stealth', shorten >= 2pt, line width = 0.5pt, node distance = 4cm]
% 		\node [circle, draw](A) {A};
% 		\node [circle,draw] (B) [right of = A] {B};
% 		\path (A) edge [bend left] node [above] {$0.4$} (B);
% 		\path (B) edge [bend left] node [below] {$0.2$} (A);
% 		\path (A) edge [loop left] node {$0.6$} (A);
% 		\path (B) edge [loop right] node {$0.8$} (B);  

% 	\end{tikzpicture}
% \end{center}
% \end{figure}
% \qs{}{

% A particle jumps between A and B, suppose this particle starts at A, what are the probability that it ends up at A after
% 	\begin{enumerate}
% 		\item one step
% 		\item n steps
% 		\item $\infty$ steps
% 	\end{enumerate}
% }
% \sol let $M = 
% 	\begin{pNiceMatrix}[light-syntax,first-row,first-col]
% 		{}	A   	B;
% 		A 	0.6 {0.2};   
% 		B 	0.4 {0.8}  
% 	\end{pNiceMatrix}
% $ 
% and $P_{0} = 
% 	\begin{pNiceMatrix}[light-syntax,first-row,first-col]
% 		{}	  ;
% 		A	 1;
% 		B	 0
% 	\end{pNiceMatrix}
% $
% where M is called a stochastic Matrix, row 1 of $P_{0}$ is 1 because the particle is initially at A.
% So to answer $(1)$, we simply multiply $M$ and $P_{0}$, let $P_{1} = MP_{0}$, we have $P_{1} = 
% 	\begin{pNiceMatrix}[light-syntax,first-row,first-col]
% 		{}	  ;
% 		A	 0.6;
% 		B	 0.4
% 	\end{pNiceMatrix}
% $
% For $(2)$, notice that we just multiply all the "states" together
% \begin{align}
% 	P_n &= MP_{n-1} \\
% 	&= M(MP_{n-2}) \\
% 	&= M \underbrace{\ldots M}_{n \textup{ times}} P_{0}= M^{n}P_{0} 
% \end{align}
% The answer to $(3)$ is a bit tricky, which we will discuss after seeing some tools to simplify our computation

% \clm{$\exists$ invertible matrix $X$ and diagnol matrix $\varLambda$ such that $M = X \varLambda X^{-1}$}{}{
% 	\begin{align*}	
% 	M &= X \varLambda X^{-1}\\
% 	M^{2} &= M(M)\\
% 	 &= (X \varLambda X^{-1})(X \varLambda X^{-1})\\
% 	 &= X \varLambda^{2} X^{-1}
% 	 \intertext{hence we must have} \\
% 	 M^{n} &= X \varLambda^{n} X^{-1}
% 	\end{align*}
% }
% This result is very useful to us because we essentially avoided calculating very large power of $M$, instead, we take the diagonal matrix $\varLambda$ to its $n$th power
% \nt{Let $\varLambda$ = 
% $\begin{pNiceMatrix}[columns-width = 1cm]
% 	\lambda_{1} & 0 \\
% 	0 & \lambda_{2}
% \end{pNiceMatrix}$
% Recall that exponent of a diagonal matrix is just its entries to the same power, so 
% $$\varLambda^{n} = \begin{pNiceMatrix}[columns-width = 1cm]
% 	\lambda_{1}^{n} & 0 \\
% 	0 & \lambda_{2}^{n}
% \end{pNiceMatrix}$$
% }
% Now, going back to $(3)$, the computation is skipped here as we haven't talked must about it yet but we will discuss more later on.
% $\varLambda$ for the problem happens to be 
% 	$\begin{pNiceMatrix}[columns-width = 1cm]
% 		1 & 0 \\
% 		0 & 0.4
% 	\end{pNiceMatrix}$
% and $X$ is 
% 	$\begin{pNiceMatrix}
% 		1 & 1 \\
% 		2 & -1	
% 	\end{pNiceMatrix}$

% $\lim\limits_{n \to \infty} M^{n}P_{0}  = \lim\limits_{n \to \infty}$
% $	\begin{pNiceMatrix}[columns-width = 1cm]
% 		1 & 1 \\
% 		2 & -1	
% 	\end{pNiceMatrix}
% 	\begin{pNiceMatrix}[columns-width = 1cm]
% 		1^{n} & 0 \\
% 		0 & 0.4^{n}
% 	\end{pNiceMatrix}$
% 	$\begin{pNiceMatrix}
% 		1 & 1 \\
% 		2 & -1	
% 	\end{pNiceMatrix}^{-1}$
% 	$\begin{pNiceMatrix}
% 		1 \\
% 		0
% 	\end{pNiceMatrix} =
% 	\begin{pNiceMatrix}
% 		0.5 \\  
% 		0.67
% 	\end{pNiceMatrix}$ 
% \\ \\So the probablity of the particle being at $A$ after infinitely many steps is roughly $0.5$
% \dfn{Limit of Sequence in $\bs{\bbR}$}{Let $\{s_n\}$ be a sequence in $\bbR$. We say $$\lim_{n\to\infty}s_n=s$$ where $s\in\bbR$ if $\forall$ real numbers $\eps>0$ $\exists$ natural number $N$ such that for $n>N$ $$s-\eps<s_n<s+\eps\text{ i.e. }|s-s_n|<\eps$$}
% \qs{}{Is the set ${x-}$axis${\setminus\{\text{Origin}\}}$ a closed set}
% \sol We have to take its complement and check whether that set is a open set i.e. if it is a union of open balls
% \nt{We will do topology in Normed Linear Space  (Mainly $\bbR^n$ and occasionally $\bbC^n$)using the language of Metric Space}
% \clm{Topology}{}{Topology is cool}
% \ex{Open Set and Close Set}{
% 	\begin{tabular}{rl}
% 		Open Set:   & $\bullet$ $\phi$                                              \\
% 		            & $\bullet$ $\bigcup\limits_{x\in X}B_r(x)$ (Any $r>0$ will do) \\[3mm]
% 		            & $\bullet$ $B_r(x)$ is open                                    \\
% 		Closed Set: & $\bullet$ $X,\ \phi$                                          \\
% 		            & $\bullet$ $\overline{B_r(x)}$                                 \\
% 		            & $x-$axis $\cup$ $y-$axis
% 	\end{tabular}}
% \thm{}{If $x\in$ open set $V$ then $\exists$ $\delta>0$ such that $B_{\delta}(x)\subset V$}
% \begin{myproof}By openness of $V$, $x\in B_r(u)\subset V$
% 	\begin{center}
% 		\begin{tikzpicture}
% 			\draw[red] (0,0) circle [x radius=3.5cm, y radius=2cm] ;
% 			\draw (3,1.6) node[red]{$V$};
% 			\draw [blue] (1,0) circle (1.45cm) ;
% 			\filldraw[blue] (1,0) circle (1pt) node[anchor=north]{$u$};
% 			\draw (2.9,0.4) node[blue]{$B_r(u)$};
% 			\draw [green!40!black] (1.7,0) circle (0.5cm) node [yshift=0.7cm]{$B_{\delta}(x)$} ;
% 			\filldraw[green!40!black] (1.7,0) circle (1pt) node[anchor=west]{$x$};
% 		\end{tikzpicture}
% 	\end{center}

% 	Given $x\in B_r(u)\subset V$, we want $\delta>0$ such that $x\in B_{\delta} (x)\subset B_r(u)\subset V$. Let $d=d(u,x)$. Choose $\delta $ such that $d+\delta<r$ (e.g. $\delta<\frac{r-d}{2}$)

% 	If $y\in B_{\delta}(x)$ we will be done by showing that $d(u,y)<r$ but $$d(u,y)\leq d(u,x)+d(x,y)<d+\delta<r$$
% \end{myproof}

% \cor{}{By the result of the proof, we can then show...}
% \mlenma{}{Suppose $\vec{v_1}, \dots, \vec{v_n} \in \RR[n]$ is subspace of $\RR^n$.}

% \mprop{}{$1 + 1 = 2$.}

% \section{Random}
% \dfn{Normed Linear Space and Norm $\boldsymbol{\|\cdot\|}$}{Let $V$ be a vector space over $\bbR$ (or $\bbC$). A norm on $V$ is function $\|\cdot\|\ V\to \bbR_{\geq 0}$ satisfying \begin{enumerate}[label=\bfseries\tiny\protect\circled{\small\arabic*}]
% 		\item \label{n:1}$\|x\|=0 \iff x=0$ $\forall$ $x\in V$
% 		\item \label{n:2}	$\|\lambda x\|=|\lambda|\|x\|$ $\forall$ $\lambda\in\bbR$(or $\bbC$), $x\in V$
% 		\item \label{n:3} $\|x+y\| \leq \|x\|+\|y\|$ $\forall$ $x,y\in V$ (Triangle Inequality/Subadditivity)
% 	\end{enumerate}And $V$ is called a normed linear space.

% 	$\bullet $ Same definition works with $V$ a vector space over $\bbC$ (again $\|\cdot\|\to\bbR_{\geq 0}$) where \ref{n:2} becomes $\|\lambda x\|=|\lambda|\|x\|$ $\forall$ $\lambda\in\bbC$, $x\in V$, where for $\lambda=a+ib$, $|\lambda|=\sqrt{a^2+b^2}$ }


% \ex{$\bs{p-}$Norm}{\label{pnorm}$V={\bbR}^m$, $p\in\bbR_{\geq 0}$. Define for $x=(x_1,x_2,\cdots,x_m)\in\bbR^m$ $$\|x\|_p=\Big(|x_1|^p+|x_2|^p+\cdots+|x_m|^p\Big)^{\frac1p}$$(In school $p=2$)}
% \textbf{Special Case $\bs{p=1}$}: $\|x\|_1=|x_1|+|x_2|+\cdots+|x_m|$ is clearly a norm by usual triangle inequality. \par
% \textbf{Special Case $\bs{p\to\infty\ (\bbR^m$ with $\|\cdot\|_{\infty})}$}: $\|x\|_{\infty}=\max\{|x_1|,|x_2|,\cdots,|x_m|\}$\\
% For $m=1$ these $p-$norms are nothing but $|x|$.
% Now exercise
% \qs{}{\label{exs1}Prove that triangle inequality is true if $p\geq 1$ for $p-$norms. (What goes wrong for $p<1$ ?)}
% \sol{\textbf{For Property \ref{n:3} for norm-2}	\subsubsection*{\textbf{When field is $\bbR:$}} We have to show\begin{align*}
% 		         & \sum_i(x_i+y_i)^2\leq \left(\sqrt{\sum_ix_i^2} +\sqrt{\sum_iy_i^2}\right)^2                                       \\
% 		\implies & \sum_i (x_i^2+2x_iy_i+y_i^2)\leq \sum_ix_i^2+2\sqrt{\left[\sum_ix_i^2\right]\left[\sum_iy_i^2\right]}+\sum_iy_i^2 \\
% 		\implies & \left[\sum_ix_iy_i\right]^2\leq \left[\sum_ix_i^2\right]\left[\sum_iy_i^2\right]
% 	\end{align*}So in other words prove $\langle x,y\rangle^2 \leq \langle x,x\rangle\langle y,y\rangle$ where
% 	$$\langle x,y\rangle =\sum\limits_i x_iy_i$$

% 	\begin{note}
% 		\begin{itemize}
% 			\item $\|x\|^2=\langle x,x\rangle$
% 			\item $\langle x,y\rangle=\langle y,x\rangle$
% 			\item $\langle \cdot,\cdot\rangle$ is $\bbR-$linear in each slot i.e. \begin{align*}
% 				      \langle rx+x',y\rangle=r\langle x,y\rangle+\langle x',y\rangle	\text{ and similarly for second slot}
% 			      \end{align*}Here in $\langle x,y\rangle$ $x$ is in first slot and $y$ is in second slot.
% 		\end{itemize}
% 	\end{note}Now the statement is just the Cauchy-Schwartz Inequality. For proof $$\langle x,y\rangle^2\leq \langle x,x\rangle\langle y,y\rangle $$ expand everything of $\langle x-\lambda y,x-\lambda y\rangle$ which is going to give a quadratic equation in variable $\lambda $ \begin{align*}
% 		\langle x-\lambda y,x-\lambda y\rangle & =\langle x,x-\lambda y\rangle-\lambda\langle y,x-\lambda y\rangle                                       \\
% 		                                       & =\langle x ,x\rangle -\lambda\langle x,y\rangle -\lambda\langle y,x\rangle +\lambda^2\langle y,y\rangle \\
% 		                                       & =\langle x,x\rangle -2\lambda\langle x,y\rangle+\lambda^2\langle y,y\rangle
% 	\end{align*}Now unless $x=\lambda y$ we have $\langle x-\lambda y,x-\lambda y\rangle>0$ Hence the quadratic equation has no root therefore the discriminant is greater than zero.

% 	\subsubsection*{\textbf{When field is $\bbC:$}}Modify the definition by $$\langle x,y\rangle=\sum_i\overline{x_i}y_i$$Then we still have $\langle x,x\rangle\geq 0$}

% \section{Algorithms}
% \begin{algorithm}[H]
% \KwIn{This is some input}
% \KwOut{This is some output}
% \SetAlgoLined
% \SetNoFillComment
% \tcc{This is a comment}
% \vspace{3mm}
% some code here\;
% $x \leftarrow 0$\;
% $y \leftarrow 0$\;
% \uIf{$ x > 5$} {
%     x is greater than 5 \tcp*{This is also a comment}
% }
% \Else {
%     x is less than or equal to 5\;
% }
% \ForEach{y in 0..5} {
%     $y \leftarrow y + 1$\;
% }
% \For{$y$ in $0..5$} {
%     $y \leftarrow y - 1$\;
% }
% \While{$x > 5$} {
%     $x \leftarrow x - 1$\;
% }
% \Return Return something here\;
% \caption{what}
% \end{algorithm}

\end{document}
